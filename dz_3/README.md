# Домашнее задание: Классификация 128x128 + U-Net c бэкбоном

## Датасеты
- Классификация: Tiny ImageNet-200 из семинара 2.
  - Набор данных необходимо взять из второго семинара и привести к разрешению 128x128
  - Разрешение: приведите вход к 128×128 (ресайз/кроп по вашему выбору, главное — корректно описать в отчёте).
- Сегментация: MOON_SEGMENTATION_BINARY из семинара 3.
  - Структура: `images/render/*.png` — входы, `images/ground/*.png` — бинарные маски.

## Часть 1. Классификатор 128×128
- Требования к модели:
  - Своя архитектура (разрешается вдохновляться CNN-идеями: Conv-BN-ReLU, блоки с даунсемплингом, GAP и т.д.).
  - Ограничение на параметризованность: не более ~5M параметров.
  - Вход: 128×128×3.
  - Можно выбрать любое количество классов для обучения!
- Обучение:
  - Трен/вал сплит: выберите собственную стратификацию.
  - Аугментации: разумные (кроп, флипы, color jitter и т.д.), кратко опишите в отчёте.
- Что сдаём по части 1:
  - График/лог обучения (loss/accuracy по эпохам) и итоговые метрики на валидации.
  - Чекпоинт модели (по возможности) или веса/ссылка.

## Часть 2. Базовая U-Net на "Луне"
- Требования к архитектуре U-Net (зафиксируйте и опишите в отчёте):
  - Глубина: 4 уровня down/4 up (энкодер-декодер с skip-связями).
  - Базовые каналы на первом уровне: 32 или 64 (по желанию можно выбрать свое количество каналов, но надо будет объяснить).
  - Даунсемплинг: stride 2 или MaxPool.
  - Итоговый слой: 1 канал с сигмоидой (бинарная сегментация).
  - Ограничение на количество параметров: до ~2.5M (рекомендуется укладываться, но допускается ±10%).
  - Вход: 128×128×3.
- Обучение:
  - Лосс: BCEWithLogitsLoss или Dice Loss, допустимо комбинировать (например, 0.5*BCE + 0.5*Dice).
  - Метрики для мониторинга: IoU (Jaccard), Dice, Pixel Accuracy.
  - Аугментации: горизонтальные/вертикальные флипы, лёгкие геометрические и цветовые — по желанию (можно добавлять другие).
- Что сдаём по части 2:
  - Логи обучения и итоговые метрики на валидации: IoU, Dice, Pixel Acc.
  - 3–5 визуализаций: вход, предсказанная маска, GT маска.

## Часть 3. U-Net с бэкбоном из классификатора
- Идея: использовать энкодер из вашей модели классификации как бэкбон U‑Net.
  - Вариант A (заморозка): заморозить веса энкодера, обучать только декодер.
  - Вариант B (тонкая настройка): полностью разморозить энкодер на поздних этапах.
  - Если у классификатора есть GAP/FC-голова — удалите её; возьмите сверточные блоки до глобального усреднения как энкодерные стадии.
- Ограничения и замечания:
  - Если количество стадий не совпадает, добавьте адаптационные 1×1 свёртки и интерполяции/пулинг, но опишите это.
- Обучение:
  - Тот же датасет "луна", те же метрики.
  - Сравните результаты с базовой U‑Net из части 2 (таблица метрик).
- Что сдаём по части 3:
  - Метрики и сравнение с базовой U‑Net (желательно таблица).